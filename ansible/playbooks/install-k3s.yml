---
- name: Install k3s on Raspberry Pi
  hosts: raspberry_pi
  become: true
  vars:
    k3s_version: "" # Empty for latest, or specify like "v1.28.5+k3s1"
    k3s_token: "" # Auto-generated if empty
    # Default to full FQDN (node-0.eldertree.local), but allow override
    k3s_hostname: "{{ k3s_hostname_override | default(inventory_hostname + '.eldertree.local') }}" # Hostname for TLS SAN
    kubeconfig_path: "~/.kube/config-eldertree" # Local path to save kubeconfig (eldertree is cluster name)
    k3s_install_k9s: true # Install k9s CLI tool
    k3s_role: "{{ k3s_role_override | default('worker') }}" # 'control-plane' or 'worker'
    k3s_ha_control_plane: "{{ k3s_ha_control_plane_override | default('node-1.eldertree.local') }}" # Existing control plane for HA join

  tasks:
    # =========================================================================
    # Check k3s installation status
    # =========================================================================
    - name: Check if k3s is already installed
      command: which k3s
      register: k3s_check
      changed_when: false
      failed_when: false

    - name: Check if k3s service is running
      systemd:
        name: k3s
      register: k3s_service_status
      changed_when: false
      failed_when: false
      when: k3s_check.rc == 0

    - name: Set fact for k3s already installed
      set_fact:
        k3s_already_installed: "{{ k3s_check.rc == 0 and k3s_service_status.status.ActiveState == 'active' }}"
      when: k3s_check.rc == 0
      changed_when: false

    - name: Set fact for k3s already installed (when not found)
      set_fact:
        k3s_already_installed: false
      when: k3s_check.rc != 0
      changed_when: false

    # =========================================================================
    # System Prerequisites - Cgroup Configuration
    # =========================================================================
    - name: Detect boot device (SD card or NVMe)
      shell: |
        ROOT_DEV=$(df / | tail -1 | awk '{print $1}')
        if echo "$ROOT_DEV" | grep -q "mmcblk"; then
          echo "sd"
        elif echo "$ROOT_DEV" | grep -q "nvme"; then
          echo "nvme"
        else
          echo "unknown"
        fi
      register: boot_device
      changed_when: false

    - name: Find boot partition and cmdline.txt location
      shell: |
        ROOT_DEV=$(df / | tail -1 | awk '{print $1}')
        if echo "$ROOT_DEV" | grep -q "mmcblk0p2"; then
          # SD card boot - standard location
          echo "/boot/firmware/cmdline.txt"
        elif echo "$ROOT_DEV" | grep -q "nvme0n1p2"; then
          # NVMe boot - need to mount boot partition
          mkdir -p /mnt/nvme-boot
          mount /dev/nvme0n1p1 /mnt/nvme-boot 2>/dev/null || true
          echo "/mnt/nvme-boot/cmdline.txt"
        else
          # Fallback to standard location
          echo "/boot/firmware/cmdline.txt"
        fi
      register: cmdline_path
      changed_when: false

    - name: Set fact for cmdline.txt path
      set_fact:
        cmdline_file: "{{ cmdline_path.stdout }}"
      changed_when: false

    - name: Read current cmdline.txt
      slurp:
        src: "{{ cmdline_file }}"
      register: cmdline_content
      changed_when: false
      failed_when: false

    - name: Check if cgroup setting exists in cmdline.txt
      set_fact:
        cgroup_setting_exists: "{{ cmdline_content.content | b64decode | default('') | regex_search('systemd\\.unified_cgroup_hierarchy=0') is not none }}"
      changed_when: false

    - name: Check if cgroup setting exists in running kernel
      shell: cat /proc/cmdline | grep -q 'systemd.unified_cgroup_hierarchy=0' && echo 'yes' || echo 'no'
      register: cgroup_running_check
      changed_when: false
      failed_when: false

    - name: Backup and update cmdline.txt with cgroup setting (safely)
      shell: |
        set -e
        CMDLINE_FILE="{{ cmdline_file }}"

        # Backup current file (if it exists and has content)
        if [ -f "$CMDLINE_FILE" ] && [ -s "$CMDLINE_FILE" ]; then
          BACKUP_FILE="${CMDLINE_FILE}.bak.ansible-$(date +%s)"
          cp "$CMDLINE_FILE" "$BACKUP_FILE"
        fi

        # Use current kernel cmdline as source of truth (it has all correct parameters)
        KERNEL_CMDLINE=$(cat /proc/cmdline)

        # Check if cgroup setting already exists in kernel
        if echo "$KERNEL_CMDLINE" | grep -q "systemd.unified_cgroup_hierarchy=0"; then
          # Already active, just ensure cmdline.txt matches
          if [ -f "$CMDLINE_FILE" ]; then
            echo "$KERNEL_CMDLINE" > "$CMDLINE_FILE"
          fi
          echo "Cgroup setting already active in kernel, cmdline.txt synced"
          exit 0
        fi

        # Add cgroup setting to kernel cmdline (preserve all existing parameters)
        NEW_CMDLINE="${KERNEL_CMDLINE} systemd.unified_cgroup_hierarchy=0"

        # Clean up extra spaces
        NEW_CMDLINE=$(echo "$NEW_CMDLINE" | sed 's/  */ /g' | sed 's/^ //' | sed 's/ $//')

        # Write to cmdline.txt (kernel cmdline is source of truth, so this is safe)
        echo "$NEW_CMDLINE" > "$CMDLINE_FILE"

        echo "SUCCESS: cmdline.txt updated from kernel cmdline with cgroup setting"
      register: cmdline_updated
      changed_when: cmdline_updated.rc == 0 and 'SUCCESS' in cmdline_updated.stdout
      failed_when: cmdline_updated.rc != 0
      when: not cgroup_setting_exists | default(false)

    - name: Unmount NVMe boot partition if mounted
      shell: |
        if mountpoint -q /mnt/nvme-boot; then
          umount /mnt/nvme-boot
          rmdir /mnt/nvme-boot
        fi
      changed_when: false
      failed_when: false
      when: boot_device.stdout == "nvme"

    - name: Set fact for reboot needed
      set_fact:
        reboot_needed: "{{ (cmdline_updated.changed | default(false)) or (cgroup_running_check.stdout | default('no') == 'no') }}"
      changed_when: false

    - name: Reboot if cmdline.txt was updated or cgroup not active
      reboot:
        msg: "Rebooting to apply cgroup configuration"
        reboot_timeout: 300
      when: reboot_needed | default(false)

    - name: Wait for system to come back online after reboot
      wait_for_connection:
        timeout: 300
      when: reboot_needed | default(false)

    - name: Verify cgroup setting is active after reboot
      shell: cat /proc/cmdline | grep -q 'systemd.unified_cgroup_hierarchy=0' && echo 'yes' || echo 'no'
      register: cgroup_verify
      changed_when: false
      failed_when: false
      when: reboot_needed | default(false)

    - name: Verify cgroup setting is active (final check)
      shell: cat /proc/cmdline | grep -q 'systemd.unified_cgroup_hierarchy=0' && echo 'yes' || echo 'no'
      register: cgroup_final_check
      changed_when: false
      failed_when: false

    - name: Fail if cgroup setting is not active after reboot
      fail:
        msg: "Cgroup setting systemd.unified_cgroup_hierarchy=0 is not active in kernel after reboot. Check {{ cmdline_file | default('/boot/firmware/cmdline.txt') }} and ensure the setting is present."
      when: reboot_needed | default(false) and cgroup_verify.stdout | default('no') == 'no'

    - name: Fail if cgroup setting is not active (general check)
      fail:
        msg: "Cgroup setting systemd.unified_cgroup_hierarchy=0 is not active in kernel. A reboot may be required."
      when: not reboot_needed | default(false) and cgroup_final_check.stdout | default('no') == 'no'

    # =========================================================================
    # Install prerequisites
    # =========================================================================
    - name: Install prerequisites
      apt:
        name:
          - curl
          - iptables
        state: present
        update_cache: true
      when: not (k3s_already_installed | default(false))

    # =========================================================================
    # Generate k3s token if not provided
    # =========================================================================
    - name: Generate k3s token if not provided
      command: openssl rand -hex 32
      register: generated_token
      changed_when: false
      when: k3s_token == "" and not (k3s_already_installed | default(false))

    - name: Set k3s token
      set_fact:
        k3s_token: "{{ generated_token.stdout if k3s_token == '' else k3s_token }}"
      when: not (k3s_already_installed | default(false))

    # =========================================================================
    # Determine if this is first control plane or additional
    # =========================================================================
    - name: Check if this is first control plane node
      set_fact:
        is_first_control_plane: "{{ inventory_hostname == 'node-1' and k3s_role == 'control-plane' }}"
      changed_when: false

    - name: Get existing control plane endpoint for HA join
      set_fact:
        ha_control_plane_endpoint: "https://{{ k3s_ha_control_plane }}:6443"
      changed_when: false
      when: k3s_role == 'control-plane' and not is_first_control_plane

    # =========================================================================
    # Install k3s - Control Plane
    # =========================================================================
    - name: Install k3s (first control plane with cluster-init)
      shell: |
        set -e
        export INSTALL_K3S_VERSION="{{ k3s_version }}"
        export K3S_TOKEN="{{ k3s_token }}"
        curl -sfL https://get.k3s.io | sh -s - server \
          --cluster-init \
          --write-kubeconfig-mode=644 \
          --tls-san={{ k3s_hostname }} \
          --disable servicelb
      args:
        creates: /usr/local/bin/k3s
      register: k3s_install
      when:
        - not (k3s_already_installed | default(false))
        - k3s_role == 'control-plane'
        - is_first_control_plane

    - name: Install k3s (additional control plane - HA join)
      shell: |
        set -e
        export INSTALL_K3S_VERSION="{{ k3s_version }}"
        export K3S_TOKEN="{{ k3s_token }}"
        curl -sfL https://get.k3s.io | sh -s - server \
          --server {{ ha_control_plane_endpoint }} \
          --write-kubeconfig-mode=644 \
          --tls-san={{ k3s_hostname }} \
          --disable servicelb
      args:
        creates: /usr/local/bin/k3s
      register: k3s_install
      when:
        - not (k3s_already_installed | default(false))
        - k3s_role == 'control-plane'
        - not is_first_control_plane

    # =========================================================================
    # Install k3s - Worker Node
    # =========================================================================
    - name: Install k3s (worker node)
      shell: |
        set -e
        export INSTALL_K3S_VERSION="{{ k3s_version }}"
        export K3S_TOKEN="{{ k3s_token }}"
        export K3S_URL="https://{{ k3s_ha_control_plane }}:6443"
        curl -sfL https://get.k3s.io | sh -s - agent
      args:
        creates: /usr/local/bin/k3s
      register: k3s_install
      when:
        - not (k3s_already_installed | default(false))
        - k3s_role == 'worker'

    # =========================================================================
    # Ensure k3s service is running
    # =========================================================================
    - name: Check k3s service status before starting
      systemd:
        name: k3s
      register: k3s_service_status_before
      changed_when: false
      when: k3s_check.rc == 0

    - name: Ensure k3s service is started
      systemd:
        name: k3s
        state: started
        enabled: true
      register: k3s_service_start
      when: k3s_check.rc == 0

    - name: Wait for k3s service to be active
      systemd:
        name: k3s
      register: k3s_service_wait
      until: k3s_service_wait.status.ActiveState == 'active'
      retries: 30
      delay: 2
      failed_when: false
      when: k3s_check.rc == 0

    - name: Check k3s logs if service failed to start
      shell: sudo journalctl -u k3s --no-pager -n 5 | tail -5
      register: k3s_logs_check
      changed_when: false
      failed_when: false
      when: k3s_check.rc == 0 and k3s_service_wait.status.ActiveState | default('') != 'active'

    - name: Display k3s logs if service failed
      debug:
        var: k3s_logs_check.stdout_lines
      when: k3s_check.rc == 0 and k3s_service_wait.status.ActiveState | default('') != 'active'

    - name: Wait for k3s kubeconfig file to exist
      wait_for:
        path: /etc/rancher/k3s/k3s.yaml
        timeout: 120
      register: kubeconfig_wait
      failed_when: false
      when: k3s_check.rc == 0

    - name: Wait for k3s to be ready (control plane)
      command: k3s kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml get nodes
      register: k3s_ready
      until: k3s_ready.rc == 0
      retries: 60
      delay: 5
      failed_when: false
      when: k3s_check.rc == 0 and k3s_role == 'control-plane'

    - name: Wait for k3s-agent to be ready (worker)
      command: systemctl is-active k3s-agent
      register: k3s_agent_ready
      until: k3s_agent_ready.rc == 0
      retries: 60
      delay: 5
      failed_when: false
      when: k3s_check.rc == 0 and k3s_role == 'worker'

    - name: Fail if k3s is not ready (control plane)
      fail:
        msg: "k3s service is not responding after 5 minutes. Check logs with: sudo journalctl -u k3s -n 50. Service status: {{ k3s_service_wait.status.ActiveState | default('unknown') }}"
      when: k3s_check.rc == 0 and k3s_role == 'control-plane' and k3s_ready.rc != 0

    - name: Fail if k3s-agent is not ready (worker)
      fail:
        msg: "k3s-agent service is not responding after 5 minutes. Check logs with: sudo journalctl -u k3s-agent -n 50"
      when: k3s_check.rc == 0 and k3s_role == 'worker' and k3s_agent_ready.rc != 0

    - name: Display k3s status (control plane)
      command: k3s kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml get nodes
      register: k3s_nodes
      changed_when: false
      failed_when: false
      when: k3s_role == 'control-plane'

    - name: Show k3s nodes (control plane)
      debug:
        var: k3s_nodes.stdout_lines
      when: k3s_nodes.rc == 0 and k3s_role == 'control-plane'

    # =========================================================================
    # Install k9s (optional)
    # =========================================================================
    - name: Check if k9s is already installed
      stat:
        path: /usr/local/bin/k9s
      register: k9s_installed_check
      changed_when: false

    - name: Get latest k9s version
      uri:
        url: https://api.github.com/repos/derailed/k9s/releases/latest
        return_content: true
      register: k9s_release
      changed_when: false
      when: k3s_install_k9s and not (k9s_installed_check.stat.exists | default(false))

    - name: Extract k9s version
      set_fact:
        k9s_version: "{{ k9s_release.json.tag_name | regex_replace('^v', '') }}"
      when: k3s_install_k9s and not (k9s_installed_check.stat.exists | default(false)) and k9s_release.json is defined

    - name: Download and install k9s
      unarchive:
        src: "https://github.com/derailed/k9s/releases/download/v{{ k9s_version }}/k9s_Linux_arm64.tar.gz"
        dest: /tmp
        remote_src: true
      register: k9s_download
      when: k3s_install_k9s and not (k9s_installed_check.stat.exists | default(false))

    - name: Move k9s to /usr/local/bin
      copy:
        src: /tmp/k9s
        dest: /usr/local/bin/k9s
        mode: "0755"
        remote_src: true
      when: k3s_install_k9s and k9s_download.changed | default(false)

    - name: Verify k9s installation
      command: k9s version
      register: k9s_version_check
      changed_when: false
      when: k3s_install_k9s

    - name: Display k9s version
      debug:
        msg: "k9s version: {{ k9s_version_check.stdout }}"
      when: k3s_install_k9s

    # =========================================================================
    # Retrieve kubeconfig and node token
    # =========================================================================
    - name: Check if local kubeconfig exists
      delegate_to: localhost
      connection: local
      become: false
      stat:
        path: "{{ kubeconfig_path | expanduser }}"
      register: local_kubeconfig_check
      changed_when: false

    - name: Wait for k3s kubeconfig file to be created
      wait_for:
        path: /etc/rancher/k3s/k3s.yaml
        timeout: 120
      register: kubeconfig_wait
      failed_when: false
      when: k3s_check.rc == 0

    - name: Check if remote kubeconfig exists
      stat:
        path: /etc/rancher/k3s/k3s.yaml
      register: remote_kubeconfig_check
      changed_when: false
      failed_when: false

    - name: Create local kubeconfig directory
      delegate_to: localhost
      connection: local
      become: false
      file:
        path: "{{ kubeconfig_path | expanduser | dirname }}"
        state: directory
        mode: "0755"

    - name: Copy kubeconfig to temp location
      copy:
        src: /etc/rancher/k3s/k3s.yaml
        dest: /tmp/k3s.yaml
        remote_src: true
        mode: "0644"
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
      register: kubeconfig_copy
      failed_when: false
      when: remote_kubeconfig_check.stat.exists | default(false)

    - name: Read kubeconfig content from Pi
      slurp:
        src: /tmp/k3s.yaml
      register: kubeconfig_content
      failed_when: false
      when: remote_kubeconfig_check.stat.exists | default(false) and not local_kubeconfig_check.stat.exists

    - name: Get current user for local file ownership
      delegate_to: localhost
      connection: local
      become: false
      command: whoami
      register: local_user
      changed_when: false

    - name: Write kubeconfig to local file
      delegate_to: localhost
      connection: local
      become: false
      copy:
        content: "{{ kubeconfig_content.content | b64decode }}"
        dest: "{{ kubeconfig_path | expanduser }}"
        mode: "0600"
        owner: "{{ local_user.stdout }}"
      register: kubeconfig_fetch
      when: kubeconfig_content.content is defined

    - name: Check if kubeconfig needs updates
      delegate_to: localhost
      connection: local
      become: false
      stat:
        path: "{{ kubeconfig_path | expanduser }}"
      register: kubeconfig_local_check
      changed_when: false

    - name: Update kubeconfig with Pi hostname
      delegate_to: localhost
      connection: local
      become: false
      replace:
        path: "{{ kubeconfig_path | expanduser }}"
        regexp: '127\.0\.0\.1'
        replace: "{{ k3s_hostname }}"
      when: kubeconfig_local_check.stat.exists | default(false)

    - name: Update kubeconfig cluster name
      delegate_to: localhost
      connection: local
      become: false
      replace:
        path: "{{ kubeconfig_path | expanduser }}"
        regexp: "name: default"
        replace: "name: {{ k3s_hostname }}"
      when: kubeconfig_local_check.stat.exists | default(false)

    - name: Update kubeconfig context cluster reference
      delegate_to: localhost
      connection: local
      become: false
      replace:
        path: "{{ kubeconfig_path | expanduser }}"
        regexp: "cluster: default"
        replace: "cluster: {{ k3s_hostname }}"
      when: kubeconfig_local_check.stat.exists | default(false)

    - name: Update kubeconfig context user reference
      delegate_to: localhost
      connection: local
      become: false
      replace:
        path: "{{ kubeconfig_path | expanduser }}"
        regexp: "user: default"
        replace: "user: {{ k3s_hostname }}"
      when: kubeconfig_local_check.stat.exists | default(false)

    - name: Update kubeconfig current-context
      delegate_to: localhost
      connection: local
      become: false
      replace:
        path: "{{ kubeconfig_path | expanduser }}"
        regexp: "current-context: default"
        replace: "current-context: {{ k3s_hostname }}"
      when: kubeconfig_local_check.stat.exists | default(false)

    - name: Set kubeconfig permissions
      delegate_to: localhost
      connection: local
      become: false
      file:
        path: "{{ kubeconfig_path | expanduser }}"
        mode: "0600"
        owner: "{{ local_user.stdout }}"
      when: kubeconfig_local_check.stat.exists | default(false)

    # =========================================================================
    # Retrieve node token
    # =========================================================================
    - name: Check if local node token exists
      delegate_to: localhost
      connection: local
      become: false
      stat:
        path: "{{ playbook_dir }}/../k3s-node-token"
      register: local_token_check
      changed_when: false

    - name: Check if remote node token exists (control plane)
      stat:
        path: /var/lib/rancher/k3s/server/node-token
      register: remote_token_check
      changed_when: false
      failed_when: false
      when: k3s_role == 'control-plane'

    - name: Download node token (control plane)
      delegate_to: localhost
      connection: local
      become: false
      fetch:
        src: /var/lib/rancher/k3s/server/node-token
        dest: "{{ playbook_dir }}/../k3s-node-token"
        flat: true
      register: token_fetch
      failed_when: false
      when:
        - not local_token_check.stat.exists
        - remote_token_check.stat.exists | default(false)
        - k3s_role == 'control-plane'

    - name: Set node token permissions
      delegate_to: localhost
      connection: local
      become: false
      file:
        path: "{{ playbook_dir }}/../k3s-node-token"
        mode: "0600"
      when: token_fetch.changed | default(false)

    # =========================================================================
    # Firewall Configuration for HA Control Plane (API server + etcd ports)
    # =========================================================================
    - name: Check if UFW is installed
      command: which ufw
      register: ufw_installed
      changed_when: false
      failed_when: false
      when: k3s_role == 'control-plane'

    - name: Ensure UFW allows k3s API server port (6443)
      ufw:
        rule: allow
        port: "6443"
        proto: tcp
        comment: "k3s API server"
      when:
        - k3s_role == 'control-plane'
        - ufw_installed.rc == 0
      register: ufw_api_server

    - name: Ensure UFW allows etcd client port (2379)
      ufw:
        rule: allow
        port: "2379"
        proto: tcp
        comment: "etcd client"
      when:
        - k3s_role == 'control-plane'
        - ufw_installed.rc == 0
      register: ufw_etcd_client

    - name: Ensure UFW allows etcd peer port (2380)
      ufw:
        rule: allow
        port: "2380"
        proto: tcp
        comment: "etcd peer"
      when:
        - k3s_role == 'control-plane'
        - ufw_installed.rc == 0
      register: ufw_etcd_peer

    - name: Ensure UFW allows etcd client from cluster network
      ufw:
        rule: allow
        from: "10.0.0.0/8"
        port: "2379"
        proto: tcp
        comment: "etcd client from cluster"
      when:
        - k3s_role == 'control-plane'
        - ufw_installed.rc == 0
      register: ufw_etcd_client_cluster

    - name: Ensure UFW allows etcd peer from cluster network
      ufw:
        rule: allow
        from: "10.0.0.0/8"
        port: "2380"
        proto: tcp
        comment: "etcd peer from cluster"
      when:
        - k3s_role == 'control-plane'
        - ufw_installed.rc == 0
      register: ufw_etcd_peer_cluster

    # =========================================================================
    # Firewall Configuration for k3s Networking (all nodes)
    # =========================================================================
    - name: Check if UFW is installed (all nodes)
      command: which ufw
      register: ufw_installed_all
      changed_when: false
      failed_when: false

    - name: Ensure UFW allows internal network (eth0)
      ufw:
        rule: allow
        from: "10.0.0.0/24"
        comment: "k3s internal network"
      when: ufw_installed_all.rc == 0

    - name: Ensure UFW allows pod network
      ufw:
        rule: allow
        from: "10.42.0.0/16"
        comment: "k3s pod network"
      when: ufw_installed_all.rc == 0

    - name: Ensure UFW allows service network
      ufw:
        rule: allow
        from: "10.43.0.0/16"
        comment: "k3s service network"
      when: ufw_installed_all.rc == 0

    - name: Ensure UFW allows Flannel VXLAN
      ufw:
        rule: allow
        port: "8472"
        proto: udp
        comment: "k3s flannel VXLAN"
      when: ufw_installed_all.rc == 0

    # =========================================================================
    # Completion message
    # =========================================================================
    - name: Display completion message (control plane)
      debug:
        msg:
          - "{{ '✅ k3s control plane installation complete!' if not (k3s_already_installed | default(false)) else '✅ k3s control plane is already installed and running' }}"
          - ""
          - "Role: {{ k3s_role }}"
          - "{{ 'HA Join: ' + ha_control_plane_endpoint if k3s_role == 'control-plane' and not is_first_control_plane else 'Cluster Init: First control plane' if k3s_role == 'control-plane' and is_first_control_plane else '' }}"
          - "Cluster endpoint: https://{{ k3s_hostname }}:6443"
          - "Kubeconfig saved to: {{ kubeconfig_path }}"
          - "{{ 'Node token saved to: ' + playbook_dir + '/../k3s-node-token' if k3s_role == 'control-plane' else '' }}"
          - ""
          - "Next steps:"
          - "  export KUBECONFIG={{ kubeconfig_path }}"
          - "  kubectl get nodes"
      when: k3s_role == 'control-plane'

    - name: Display completion message (worker)
      debug:
        msg:
          - "{{ '✅ k3s worker installation complete!' if not (k3s_already_installed | default(false)) else '✅ k3s worker is already installed and running' }}"
          - ""
          - "Role: {{ k3s_role }}"
          - "Connected to: https://{{ k3s_ha_control_plane }}:6443"
          - ""
          - "Next steps:"
          - "  kubectl get nodes (from control plane)"
      when: k3s_role == 'worker'